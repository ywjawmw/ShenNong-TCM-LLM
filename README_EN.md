# ShenNong Series Models
ShenNong-TCM-LLM (the first Traditional Chinese Medicine large language model) Updated Version V2.0


[**ä¸­æ–‡**](./README.md) | [**English**](./README_EN.md)



<p align="center">
    <br>
    <img src="./pics/ShenNong-TCM_banner.png" width="355"/>
    <br>
</p>
<p align="center">
    <img alt="GitHub" src="https://img.shields.io/github/license/ymcui/Chinese-LLaMA-Alpaca.svg?color=blue&style=flat-square">
    <img alt="GitHub top language" src="https://img.shields.io/github/languages/top/ymcui/Chinese-LLaMA-Alpaca">
</p>


Large Language Models (LLMs) represented by ChatGPT, GPT-4, etc., have triggered a new wave of research in the field of natural language processing, demonstrating capabilities close to Artificial General Intelligence (AGI), and attracting wide attention from industry and academia.

To promote the development and application of LLMs in the field of Traditional Chinese Medicine (TCM), enhance their knowledge and ability to answer medical consultations in TCM, and advance the inheritance of TCM empowered by large models, we hereby release the **ShenNong** large-scale language model for TCM:

- ğŸš€ [ShenNong-TCM] :
    - The training data comes from [ShenNong_TCM_Dataset].
    - ChatMed_TCM_Dataset is based on our open-source [TCM Knowledge Graph](https://github.com/ywjawmw/TCM_KG);
    - Using an entity-centric self-instruct method [entity-centric self-instruct](https://github.com/ywjawmw/ShenNong-TCM-LLM/blob/main/crawl_tcm_prompt_symptom_combo.py), we called ChatGPT to generate over 110,000 instruction data centered on TCM [SN-QA](https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset);
    - ShenNong-L model is based on LlaMA 7B and fine-tuned with LoRA (rank=16). The fine-tuning code is the same as [ChatMed codebase](https://github.com/michael-wzhu/ChatMed).
    - ShenNong-Q model is based on Qwen 1.8B and fine-tuned with LoRA (rank=16).

We also welcome everyone to follow our other open-source medical LLM projects:
- ğŸš€ [Intelligent TCM Inheritance and Innovation Assistance Platform](https://github.com/ywjawmw/AI4TCM-Platform) - Intelligent TCM Inheritance and Innovation Platform;
- ğŸš€ [ChatMed-Consult](https://huggingface.co/michaelwzhu/ChatMed-Consult): Based on [Chinese medical online consultation dataset ChatMed_Consult_Dataset](https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset) with 500k+ consultation dialogues plus ChatGPT responses as training data. The backbone is [LlaMA-7b](https://github.com/facebookresearch/llama), integrated with [Chinese-LlaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) LoRA weights and Chinese extended vocabulary, then fine-tuned with LoRA. We have open-sourced all code.
- ğŸš€ [ChatMed-MT](https://huggingface.co/michaelwzhu/ChatMed-MT): A multi-turn dialogue version of ChatMed-Consult, where existing open-source Chinese consultation datasets were auto-transformed by LLM to produce more empathetic and detailed doctor responses, improving patient/user experience.
- ğŸš€ [PromptCBLUE Chinese Medical LLM Benchmark](https://github.com/michaelwzhu/PromptCBLUE): Transforming [CBLUE](https://tianchi.aliyun.com/dataset/95414) into a prompt-learning benchmark to evaluate Chinese medical knowledge and medical text processing ability of LLMs. PromptCBLUE aims to enable a single generative LLM to complete various medical NLP tasks such as medical record structuring, consultation, case writing, etc.



## Updates

2025/09/29 ğŸš€ Released [ShenNong-Q model checkpoint](https://huggingface.co/WJing123/ShenNong-Q), and updated SN-QA v0.2 self-instruct generation code along with the [inference pipeline](./reasoning_pipeline).

2023/6/25 ğŸš€ Released [ShenNong_TCM_Dataset(SN-QA)](https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset) v0.2 version with 110k+ data entries; also uploaded [ShenNong-L model checkpoint](https://huggingface.co/michaelwzhu/ShenNong-TCM-LLM). 

2023/6/21 ğŸš€ Released [ShenNong_TCM_Dataset](https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset) v0.1, v0.2 version coming soon;




## Quick Start

If you want to fine-tune a large model using [SN-QA dataset](https://huggingface.co/datasets/WJing123/SN-QA), you can refer to the code and training scripts in [ChatMed codebase](https://github.com/michael-wzhu/ChatMed);


## Entity-Centric Self-Instruct Method

The [SN-QA dataset](https://huggingface.co/datasets/WJing123/SN-QA) is fully open-source and available for community use.

We know that compared to general domains, vertical domains are usually knowledge-intensive, and this knowledge is generally centered around entities. Therefore, we propose an entity-centric self-instruct method [entity-centric self-instruct](https://github.com/ywjawmw/ShenNong-TCM-LLM/blob/main/query_crawl_tcm_prompt_symptom_combo.py), which generates instructions around core entities and various intent scenarios in the vertical domain.
If you want to generate based on the [symptom set](https://github.com/ywjawmw/ShenNong-TCM-LLM/blob/main/all_TCM_name.txt) from the *Dictionary of Chinese Medicine Prescriptions*:

```bash
python crawl_tcm_prompt_symptom_combo.py   # Advanced version with more commands
python query_crawl_tcm_prompt_symptom_combo.py  # Paper version
```
You can choose freely, and further optimize and construct on top of this~~~

If you want to generate entity-centric self-instruct based on your local knowledge base/knowledge graph, run the following command (note: configure your own API key in the code file):
```bash
python query_crawl_tcm_KG_prompts.py

```
The â€œbaseline_all_kg_triples.txtâ€ file contains triples from the knowledge graph in plain text, see TCM-KG.


Launch ShenNong for inference:
```bash
sh ./reasoning_pipeline/api.sh

```
## Effect Comparison

### Example: ShenNong-L

By fine-tuning the Chinese LLaMA-7b model with the SN-QA dataset using LoRA, the model achieves significant improvement in answering TCM-related questions. Below we show 5 typical TCM questions with responses from both models. More test cases are available [here](./ShenNong-TCM-LLM-test examples.xls).
| é—®è¯Šé—®é¢˜ | ä¸­æ–‡LlaMA-7b | ShenNong-TCM                                                                                                                                                                                                                                                                                                                               |
|------|------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| å¯ä»¥ç»™æˆ‘ä»‹ç»ä¸€äº›æ²»ç–—å¤ªé˜³ç»é£å¯’è¯çš„ä¸­è¯å—ï¼Ÿ    |    é’ˆç¸ã€æ¨æ‹¿ç­‰ä¸­åŒ»ç–—æ³•ï¼Œä»¥åŠè‰è¯æ±¤å‰‚éƒ½å¯ä»¥ç”¨æ¥ç¼“è§£è¿™ç§ç—‡çŠ¶ã€‚        | å½“å‡ºç°å¤ªé˜³ç»é£å¯’è¯æ—¶ï¼Œå¯ä»¥è€ƒè™‘ä»¥ä¸‹å‡ ç§å¸¸ç”¨çš„ä¸­è¯è¿›è¡Œæ²»ç–—ã€‚è¿™äº›è¯ç‰©å…·æœ‰æ¸©é˜³æ•£å¯’ã€ç¥›æ¹¿æ­¢ç—›çš„ä½œç”¨ï¼Œèƒ½å¤Ÿç¼“è§£ç—‡çŠ¶å¹¶ä¿ƒè¿›èº«ä½“åº·å¤ã€‚\n\n1. å½“å½’ï¼ˆAngelica sinensisï¼‰-æœ‰æ´»è¡€åŒ–ç˜€å’Œè¡¥æ°”å…»è¡€ä½œç”¨ï¼›\n2. ç™½èŠ· (Bai Zi) - æ¸…çƒ­è§£æ¯’ï¼Œå®£é€šé¼»çªï¼›\n3. é‡‘é“¶èŠ±(Honeysuckle, Jin Xin Hua)-æ¸…çƒ­è§£æ¯’ï¼Œå‡‰è¡€æ¶ˆè‚¿ï¼›\n4. é™ˆçš® (Chen Pi) - ç†æ°”å¥è„¾ï¼Œè¡Œæ»åŒ–ç—°ï¼›\n5. æ¡‚æ (Gui Zhi) - å‘æ±—è§£è¡¨ï¼Œè°ƒå’Œè¥å«ã€‚\n\néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨ä½¿ç”¨ä»»ä½•ä¸€ç§ä¸­è¯ä¹‹å‰åº”è¯¥å’¨è¯¢åŒ»ç”Ÿæˆ–ä¸­åŒ»å¸ˆçš„æ„è§ä»¥ç¡®ä¿å®‰å…¨æœ‰æ•ˆæ€§ã€‚                                                 |
| æˆ‘æ„Ÿè§‰æµ‘èº«ä¹åŠ›ï¼Œä¸”å‡ºç°å£®çƒ­ç—‡çŠ¶ã€‚è¯·æ¨èä¸­åŒ»è¯Šæ–­å’Œæ²»ç–—æ–¹æ³•ã€‚   |      å»ºè®®æ‚¨åˆ°åŒ»é™¢è¿›è¡Œæ£€æŸ¥ä»¥ç¡®å®šç—…å› å¹¶åˆ¶å®šç›¸åº”çš„æ²»ç–—æ–¹æ¡ˆï¼›åŒæ—¶å¯ä»¥é‡‡ç”¨ä¸­è¯è°ƒç†èº«ä½“ã€å¢å¼ºå…ç–«åŠ›ç­‰æ–¹æ³•ç¼“è§£ç—…æƒ…ã€‚     |   æ ¹æ®æ‚¨çš„æè¿°ï¼Œæ‚¨å¯èƒ½æ‚£æœ‰æ„Ÿå†’æˆ–æµæ„Ÿç­‰ç–¾ç—…å¼•èµ·çš„èº«ä½“ä¸é€‚ã€‚å»ºè®®åŠæ—¶å°±åŒ»å¹¶è¿›è¡Œä¸­è¥¿åŒ»ç–—è¯Šä»¥ç¡®å®šç¡®åˆ‡çš„ç—…å› åŠæ²»ç–—æ–¹æ¡ˆã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨çš„ä¸­è¯æ–¹å‰‚ä¾›å‚è€ƒï¼š\n\n1. é‡‘é“¶èŠ±æ¸…ç˜Ÿèƒ¶å›Šï¼šå…·æœ‰è§£æ¯’ã€æŠ—ç—…æ¯’çš„ä½œç”¨ï¼Œé€‚ç”¨äºé£å¯’æ„Ÿå†’æˆ–è€…æµè¡Œæ€§æ„Ÿå†’ç­‰ç—…ç—‡ã€‚\n\n2. ä¸‰é»„çŸ³è†é¢—ç²’ï¼šå¯æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ­¢ç—›ï¼Œå¸¸ç”¨äºæš‘æ¹¿ä¼¤é˜³æ‰€è‡´çš„èº«ä½“é…¸è½¯æ— åŠ›ã€‚\n\n3. äº”å‘³å­é¥®ç‰‡ï¼šæœ‰æ¸©è‚ºæ•£å¯’ã€ç›Šæ°”å›ºè¡¨ä¹‹åŠŸæ•ˆï¼Œå¯ç”¨äºä½“è™šç•å†·ã€æ˜“æ„Ÿæ¶é£çš„æƒ…å†µã€‚\n\n4. å½“å½’ç”Ÿå§œç¾Šè‚‰æ±¤ï¼šæœ‰ç›Šè¡€æ´»ç»œã€æ¸©æš–è„¾èƒƒä½œç”¨ï¼Œé€‚åˆå› å¯’å†·å¯¼è‡´çš„èº«ä½“ç–²å€¦ä»¥åŠé£Ÿæ¬²ä¸æŒ¯çš„äººç¾¤é£Ÿç”¨ã€‚\n\néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä»¥ä¸Šè¯ç‰©ä»…ä½œä¸ºè¾…åŠ©æ²»ç–—æªæ–½ï¼Œå…·ä½“ç”¨è¯åº”åœ¨åŒ»ç”ŸæŒ‡å¯¼ä¸‹ä½¿ç”¨ã€‚åŒæ—¶ï¼Œä¿æŒå……è¶³ä¼‘æ¯ã€é¥®é£Ÿæ¸…æ·¡ã€å¤šå–æ°´ä¹Ÿæ˜¯ç¼“è§£ç—…æƒ…çš„é‡è¦æ–¹æ³•ä¹‹ä¸€ã€‚    |   
| è…¹ç—›æ€ä¹ˆåŠå•Šï¼Ÿ    |      å»ºè®®æ‚¨å°½å¿«å°±åŒ»ï¼Œå¹¶æ ¹æ®åŒ»ç”Ÿçš„æŒ‡å¯¼è¿›è¡Œæ²»ç–—ã€‚      | å¦‚æœæ‚¨æ„Ÿåˆ°è…¹éƒ¨ç–¼ç—›ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½æœ‰ç”¨çš„å»ºè®®:\n1. é¦–å…ˆä¼‘æ¯ä¸€ä¸‹ã€‚èººä¸‹æˆ–åä¸‹æ¥å¯ä»¥å‡è½»ç—‡çŠ¶å¹¶å¸®åŠ©èº«ä½“æ¢å¤åŠ›é‡ã€‚\n2. å¦‚æœæ‚¨æœ‰ä»»ä½•è¯ç‰©è¿‡æ•å²æˆ–è€…æ­£åœ¨æœç”¨å…¶ä»–è¯å“ï¼Œè¯·åŠ¡å¿…å’¨è¯¢åŒ»ç”Ÿçš„æ„è§åå†ä½¿ç”¨è¿™äº›è¯è†ã€æ­¢ç–¼å‰‚ç­‰ç¼“è§£ç—›è‹¦çš„æ–¹æ³•ã€‚\n3. åœ¨é¥®é£Ÿæ–¹é¢ï¼Œé¿å…é£Ÿç”¨è¾›è¾£é£Ÿç‰©å’Œæ²¹è…»é£Ÿå“ï¼Œå°½é‡é€‰æ‹©æ¸…æ·¡æ˜“æ¶ˆåŒ–çš„é£Ÿç‰©æ¥å‡å°‘è‚ èƒƒè´Ÿæ‹…ã€‚åŒæ—¶å¤šå–æ°´ä»¥ä¿æŒæ°´åˆ†å¹³è¡¡ã€‚\n4. å¦‚æœä½ çš„æƒ…å†µæ¯”è¾ƒä¸¥é‡å¹¶ä¸”æŒç»­æ—¶é—´è¾ƒé•¿ï¼Œæœ€å¥½å»åŒ»é™¢å°±è¯Šå¯»æ±‚ä¸“ä¸šæ²»ç–—ã€‚                                                                                                                        |
| å¤§ä¾¿æ—¶å¸¦è¡€ï¼Œæ€ä¹ˆåŠï¼Ÿ    |     å‡ºç°è¿™ç§æƒ…å†µå¯èƒ½æ˜¯ç”±äºè‚ é“å†…å‡ºè¡€æˆ–ç‚ç—‡å¼•èµ·çš„ã€‚å»ºè®®åŠæ—¶å°±åŒ»æ£€æŸ¥ç¡®è¯Šå¹¶è¿›è¡Œæ²»ç–—ã€‚       | å¦‚æœæ‚¨çš„å¤§ä¾¿ä¸­å‡ºç°äº†è¡€æ¶²ï¼Œå¯èƒ½æ˜¯ç”±äºä»¥ä¸‹åŸå› ä¹‹ä¸€å¼•èµ·çš„ï¼š\n\n1.ç—”ç–®æˆ–è‚›è£‚ - è¿™ç§æƒ…å†µé€šå¸¸ä¼šå¯¼è‡´æ’å°¿å’Œæ’æ³„ç‰©çš„ç–¼ç—›ã€ç˜™ç—’å’Œå…¶ä»–ä¸é€‚ç—‡çŠ¶ã€‚å‡ºè¡€é‡å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œä»è½»å¾®åˆ°ä¸¥é‡ä¸ç­‰ã€‚\n2.è‚ ç‚æˆ–å…¶ä»–è‚ é“ç–¾ç—…- è¿™äº›ç—…ç—‡å¯èƒ½å¯¼è‡´è…¹æ³»æˆ–è€…ä¾¿ç§˜ä»¥åŠå…¶ä»–èƒƒè‚ é“é—®é¢˜ã€‚å‡ºè¡€é‡ä¹Ÿå¯èƒ½å› ç—…æƒ…è€Œå¼‚ã€‚\n3.ç›´è‚ æ¯è‚‰ - è¿™æ˜¯ä¸€ç§åœ¨ç›´è‚ å†…å½¢æˆçš„è‰¯æ€§è‚¿ç˜¤çŠ¶ç»“æ„ï¼Œå¯ä»¥å¼•èµ·ä¸åŒç¨‹åº¦çš„è…¹ç—›å’Œå‡ºè¡€ã€‚\n4.ç»“è‚ ç™Œ - è¿™ç§ç™Œç—‡æ˜¯å¸¸è§çš„æ¶ˆåŒ–ç³»ç»Ÿæ¶æ€§è‚¿ç˜¤ç±»å‹ä¹‹ä¸€ï¼Œå¸¸å¸¸ä¼šå¼•èµ·ä¸¥é‡çš„è…¹éƒ¨ç—›è‹¦å¹¶ä¼´æœ‰è´«è¡€ç­‰é—®é¢˜ã€‚\n5.è„±å‚å­å®«é¢ˆç³œçƒ‚ - å¦‚æœæ‚¨æ­£åœ¨æ€€å­•å¹¶ä¸”å‡ºç°è¿™ç§æƒ…å†µï¼Œè¿™å¾ˆå¯èƒ½æ˜¯ç”±äºå®«é¢ˆç³œçƒ‚æ‰€è‡´ã€‚è¿™ç§ç—…å˜éœ€è¦åŠæ—¶æ²»ç–—ä»¥é¿å…æ„ŸæŸ“å’Œæµäº§çš„é£é™©ã€‚\n\nå»ºè®®ç«‹å³å°±åŒ»å¯»æ±‚åŒ»ç”Ÿå¸®åŠ©è¿›è¡Œè¯Šæ–­å’Œæ²»ç–—ã€‚  |
| æˆ‘ç‰™é½¿ç–¼ç—›ï¼ŒåŒæ—¶ä¼´æœ‰å£è‡­ï¼Œæœ‰ä»€ä¹ˆæ–¹æ³•ç¼“è§£ï¼Ÿ    |      å»ºè®®æ‚¨åŠæ—¶åˆ°å£è…”åŒ»é™¢å°±è¯Šã€‚ç‰™å‘¨ç‚ã€é¾‹é½¿ç­‰ç–¾ç—…å¯èƒ½å¯¼è‡´ç‰™é½¿ç—›å’Œå£è‡­ç­‰é—®é¢˜å‡ºç°     | æ‚¨å¯ä»¥å°è¯•ä»¥ä¸‹å‡ ç§æ–¹å¼æ¥å‡è½»ç‰™ç—›å’Œå‡å°‘å£è…”å¼‚å‘³ï¼š\n\n1.æ¼±å£ç›æ°´ã€‚å°†ä¸€èŒ¶åŒ™é£Ÿé†‹åŠ å…¥ä¸€æ¯æ¸©æ°´ä¸­ï¼Œæ…æ‹Œå‡åŒ€åç”¨æ¥æ¼±å£å³å¯ã€‚è¿™æœ‰åŠ©äºæ€æ­»ç»†èŒå¹¶å‡è½»å£æ°”ã€‚\n\n2.ä½¿ç”¨å«æ°Ÿçš„æ´—å‘å‰‚æˆ–å»å±‘äº§å“ã€‚è¿™äº›äº§å“çš„æˆåˆ†å«æœ‰ä¸€å®šé‡çš„æ°ŸåŒ–ç‰©ï¼Œå¯å¸®åŠ©æ¸…æ´ç‰™é½¿è¡¨é¢ï¼Œé˜²æ­¢é¾‹é½¿å½¢æˆã€‚\n\n3.åˆ·ç‰™æ—¶ç”¨è½¯æ¯›åˆ·è½»è½»æŒ‰æ‘©ç‰™é½¿è¾¹ç¼˜ä»¥å»é™¤é£Ÿç‰©æ®‹æ¸£ã€‚\n\n4.å®šæœŸå»çœ‹ç‰™åŒ»è¿›è¡Œæ£€æŸ¥å’Œæ²»ç–—ã€‚å¦‚æœå‡ºç°ä¸¥é‡çš„è›€æ´æˆ–å…¶ä»–é—®é¢˜ï¼ŒåŠæ—¶ä¿®å¤å¯èƒ½ä¼šé¿å…æ›´ä¸¥é‡åæœã€‚\n\n5.æ³¨æ„é¥®é£Ÿå¥åº·ï¼Œå°‘åƒç”œé£Ÿã€ç¢³é…¸é¥®æ–™ç­‰æ˜“å¯¼è‡´è›€ç‰™çš„é£Ÿç‰©ã€‚|


We can see:
	â€¢	ShenNong-Lâ€™s answers are more humane and caring, while the Chinese LlaMA-7b modelâ€™s responses may be relatively stiff;
	â€¢	Compared with the Chinese LlaMA-7b model, ShenNong-Lâ€™s answers are richer, with more practical suggestions, especially effectively recommending formulas/herbs based on symptoms.

## Disclaimer
	â€¢	The resources related to this project are for academic research only and strictly prohibited for commercial use.
	â€¢	As an intelligent assistant based on language models, ShenNong-TCM cannot replace TCM/Western medical diagnosis or provide medical advice. If needed, please consult a professional doctor or go to a hospital.
	â€¢	The ShenNong-TCM series of models are iterating rapidly, and model weights will be updated regularly.
	â€¢	The ShenNong-TCM series is based on open-source data, whose quality and quantity are limited; its TCM knowledge inevitably has various shortcomings. We will continue to improve and update it.

## Acknowledgments

This project is developed based on open-source projects. We would like to thank the related projects and researchers.
- [ShenNong-TCM-LLM,V1.0](https://github.com/michael-wzhu/ShenNong-TCM-LLM)
- [LlaMA](https://github.com/facebookresearch/llama)
- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
- [Chinese-LlaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)
- [ChatMed](https://github.com/michael-wzhu/ChatMed)
- [Qwen](https://huggingface.co/Qwen)

The â€œShenNongâ€ image in the logo was automatically generated by [midjourney](http://midjourney.com).

